---
title: "Prediction Assignment Writeup"
author: "Luiz Fernando da Silva"
date: "August, 2015"
output: html_document
---

Currently is possible to get a large volume of data about activities of our daily lives in a simply and cheaply way. In one study, accelerometer were placed on the belt, arm and barbells of 6 participants, they performed a movement in 5 different ways. This document seeks to create a model able to predict how a person is doing the same exercise.

```{r}
library(caret)
library(randomForest)
```


## Loading the data

The data is contained in two CSV files, one for the training data, which includes the variable `classe`, and another for testing, where the algorithm should provide what is the `classe`'.

```{r cache=TRUE}
set.seed(1910)
training <- read.csv("pml-training.csv", header=TRUE, sep=",", na.strings=c("NA", "", "#DIV/0!"))
testing <- read.csv("pml-testing.csv", header=TRUE, sep=",", na.strings=c("NA", "", "#DIV/0!"))
```

Let's check how many records exist for each `classe`:

```{r}
summary(training$classe)
```

Since there are many variables in this data set we will remove the columns with close to zero variance:

```{r}
near_zero <- nearZeroVar(training,saveMetrics=TRUE)
training <- training[, !near_zero$nzv]
testing <- testing[, !near_zero$nzv]
```

There are still many variables with missing values, we will remove those that have less than 51% of data:

```{r}
lowDataCols <- c((colSums(!is.na(training[,-ncol(training)])) >= 0.51 * nrow(training)))
training <- training[, lowDataCols]
testing <- testing[, lowDataCols]
```

Other variables can influence the outcome but has no real importance, we will remove them also:

```{r}
drops <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", 
     "num_window")
training <- training[, !(names(training) %in% drops)]
testing <- testing[, !(names(testing) %in% drops)]
```

Now let's split the data preprocessed into training and validation sets:

```{r}
inTrain <- createDataPartition(y=training$classe, p=0.7, list=FALSE)
final_training <- training[inTrain,]
final_validation <- training[inTrain,]
```

## Creating the model

To get a better accuracy in prediction will use randomForest algorithm.

```{r cache=TRUE}
model <- randomForest(classe~., data=final_training)
```

We will validate the model:

```{r}
validation <- predict(model, newdata=final_validation)
confusionMatrix(validation, final_validation$classe)
```

Let's check the accuracy of the model:

```{r}
accuracy <- c(as.numeric(validation==final_validation$classe))
accuracy <- sum(accuracy) * 100 / nrow(final_validation)
```

We got a precision in the test of **`r accuracy`%**.

```{r}
plot(model, lty=c(1,1,1,1,1,1), main="Estimated Error by Number of Trees")
```

```{r fig.height=8}
varImpPlot(model, main="Predictors Importance")
```


## Testing the prediction

Now let's test the prediction in a new data set of 20 records:

```{r}
test <- predict(model, newdata=testing)
test
```

```{r}
test_table <- table(test)
test_table
```


For the second part of assignment:

```{r}
n = length(test)
for(i in 1:n){
  filename = paste0("problem_id_",i,".txt")
  write.table(test[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
```




